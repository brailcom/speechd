\input texinfo   @c -*-texinfo-*-
@c %**start of header
@setfilename speechd.info
@settitle Speech Dispatcher --- Easy speech synthesis
@finalout
@c @setchapternewpage odd
@c %**end of header

@syncodeindex pg cp
@syncodeindex fn cp
@syncodeindex vr cp

@dircategory Sound
@dircategory Development

@direntry
* Speech Dispatcher: (speechd).       Speech Dispatcher.
@end direntry

@titlepage
@title Speech Dispatcher --- Easy speech synthesis
@subtitle Mastering the Babylon of TTS'
@subtitle for Speech Dispatcher @value{VERSION}
@author Tom@'a@v{s} Cerha <@email{cerha@@brailcom.org}>
@author Hynek Hanke <@email{hanke@@volny.cz}>
@author Milan Zamazal <@email{pdm@@brailcom.org}>

@page
@vskip 0pt plus 1filll

This manual documents Speech Dispatcher, version @value{VERSION}.

Copyright @copyright{} 2001, 2002, 2003 Brailcom, o.p.s.

@quotation
Permission is granted to copy, distribute and/or modify this document
under the terms of the GNU Free Documentation License, Version 1.2
or any later version published by the Free Software Foundation;
with no Invariant Sections, no Front-Cover Texts, and no Back-Cover Texts.
A copy of the license is included in the section entitled "GNU
Free Documentation License".
@end quotation

@end titlepage

@c So the toc is printed in the right place.
@contents

@ifnottex
@node Top, Introduction, (dir), (dir)

This file documents the @code{speechd} client/server application 
that attempts to provide a common interface to different synthesizers.

This manual documents Speech Dispatcher, version @value{VERSION}.

Copyright @copyright{} 2001, 2002, 2003 Brailcom, o.p.s.

@quotation
Permission is granted to copy, distribute and/or modify this document
under the terms of the GNU Free Documentation License, Version 1.2
or any later version published by the Free Software Foundation;
with no Invariant Sections, no Front-Cover Texts, and no Back-Cover Texts.
A copy of the license is included in the section entitled "GNU
Free Documentation License".
@end quotation
@end ifnottex

@menu
* Introduction::                What is Speech Dispatcher.
* Internal structure::          How does Speech Dispatcher work.
* Public API::                  How to use Speech Dispatcher in your programs.
* SSIP::                        Speech Synthesis Internet Protocol.
* Priorities::                  Description, guidelines how to use them.
* Multiple output modules::     Using different synthesizers.
* Message history::             Handling history of arrived messages.
* Speech parameters::           Settings that affect the way Speech Dispatcher speaks.
* Configuration::               How to configure Speech Dispatcher.

* Key names::                   List of the symbolic key names.
* Standard sound icons::        List of the standard sound icon names.
* Standard spelling tables::    List of the standard spelling table names.
* Standard sound tables::       List of the standard sound table names.
* Standard voices::             List of the standard voice names.

* GNU General Public License::  Copying conditions of Speech Dispatcher.
* GNU Free Documentation License::  Copying conditions of this manual.

* Concept index::               Index of concepts.
@end menu

@node Introduction, Internal structure, Top, Top
@chapter Introduction

@menu
* Why and how::                 Why Speech Dispatcher?  Philosophy, motivation...
* Current state::               Speaking software today.
* Basic design::                How does it work?
* User::                        Speech Dispatcher from the user's point of view.
* Programmer::                  Speech Dispatcher from the programmer's point of view.
@end menu

@node Why and how, Current state, Introduction, Introduction
@section Why and how
@cindex Basic ideas, Motivation
@cindex Philosophy

Speech Dispatcher project comes to provide a device independent layer for
speech synthesis. It should provide a simple interface for client
applications (applications, that want to speak) as well as for device
driver modules (for particular speech synthesis).

High quality speech synthesis has been available for a long time and now
it's usable even by ordinary users on their home PC's. It comes sometimes
as a necessity, sometimes as a good feature for programs to provide speech
output.  There is a wide field of possible uses from educational software,
through specialized systems (hospitals, laboratories, telephony servers).
For visually impaired users it is one of the two essential ways of getting
the output from computer (the second one is Braille display). That's also
where Speech Dispatcher comes from.

There are different speech synthesizers with different capabilities.  Some
of them are hardware, some of them are software.  Some of them are Free
Software and are are available on the Internet.  However, none of them is
pre-installed in one of the widely used GNU/Linux distributions.
Programmers have really hard times when they want to make their program
speak because they need to find some suitable synthesizer (long hours of
experiments and so on...) and then make it work with their program.  They
often need to write output device drivers for these programs or hardware
devices and are doing it again and again.  You can imagine it all fails
when an innocent user executes two programs with speech output at once ---
if they even start both (what I doubt), they will be shouting one over the
other.  This makes it very hard for programmers to implement speech support
to their programs (for blind users or simply to make a better user
interface) and it's one of the reasons we still don't fully exploit what
speech synthesis technology offers.

In an ideal world, programmers could use similar commands for speech
synthesis as they do for normal text output (printf, puts, ...).  In an
ideal world, there would be some speech_printf() that would take care of
saying your message in the right time without interrupting others, without
you being obligated to take care of how exactly the communication with
speech synthesizer is implemented and without you having to worry about
what synthesizer to use and if it's available.  In an ideal world, there
would be some speech synthesizer in each GNU/Linux distribution and some
speech dispatcher taking care of all applications that want to speak,
allowing user to configure speech parameters and providing simple interface
(as speech_printf()) through some shared library for programmers.  It will
be a long way until we achieve this state of things, but with Speech
Dispatcher, we are taking the first steps...

@node Current state, Basic design, Why and how, Introduction
@section Current state
@cindex Synthesizers
@cindex Other programs

Today, the development of programs and new technologies connected
with speech synthesis under GNU/Linux is centered around two main
points: visually impaired people and pure development. Although some
fields are beginning to use synthesis for different purposes, like
telephony servers, these are still like drops of water in the ocean.
Here is a short (definitely not exhaustive list) software synthesizers,
hardware synthesizers and applications known to work under GNU/Linux.

@enumerate
@item Speech Synthesizers

@itemize @bullet

@item Hardware synthesizers

Hardware synthesizers are the devices, which may be connected to PC.
Mostly they are external, connected via serial or parallel port.  There
are also some internal devices for ISA bus or USB.  Application may
send textual data to the port and the device converts it to spoken
letters and words.  Data may contain also several control sequences in
the form of escaped characters as commands.  The problem, we are
facing, is that each of these devices uses its own communication
protocol.

@item Software synthesizers
@pindex Festival
@pindex Flite
@pindex Odmluva
@pindex Epos
@pindex FreeTTS

@itemize @minus

@item Festival

Festival is a multi-lingual Free Software text to speech synthesizer
with high quality speech databases available. One of it's problems
is that some of the most important databases are not free.
(e.g. the database for British English is non-free). Other problem
is that Festival is intended rather as a platform for research
and development than as an end-user product and therefore is
big and not-so-easy to install. 

@item Flite

Flite stands for Festival Lite and it is a light fully free English
speech software synthesizer with good quality of sound, developed
by the authors of Festival as an end-user product. It seems that
the developers have some problems with importing the voices from
Festival. Speech Dispatcher currently uses Flite as it's primary
output module for English.

@item Epos

Epos is Czech synthesis. It is an academic project and it
already gives quite good results, but some parts are covered
by a proprietary license.
@c TODO: We need to add more information.

@item Free TTS

Free TTS is some JAVA-based text-to-speech system. We didn't
checked it yet.
@c TODO: check it...

@item IBM ViaVoice

ViaVoice is a multi-lingual software synthesizer available for GNU/Linux.
The main problem is that ViaVoice is not free (as in freedom). Until IBM
changes its license, we can't use it in Free World / Free Operating System
and therefore it's not and will not be supported in Speech Dispatcher.

@item MBROLA

MBROLA is a multi-lingual software synthesis, available for GNU/Linux.
MBROLA is not free as in freedom, although it's gratis. The same problems
as with IBM ViaVoice prevents us to include it in Speech Dispatcher.
(You can use MBROLA voices through Festival though, although we will
be much happier if you try to setup a new free -- as in freedom -- voice
for festival)

@end itemize
@end itemize 

@item Speaking applications

@itemize @minus

@item SpeechdSpeak

...

@item Emacspeak

The Emacspeak (by T. V. Raman <@email{raman@@cs.cornell.edu}>) software package
provides speech output for Emacs, and includes ,,speech servers'' for
the Dectalk speech synthesizers.

We had been experimenting with an Emacspeak server for Speech Dispatcher,
but for the bellow stated reasons we decided to abandon this idea and
implement Speechdspeak, which is in some respects similar to
Emacspeak, but it's written specially to best suit Speech Dispatcher
capabilities.

The Emacspeak speech servers package provides servers for several
additional synthesizers.  None of these programs are normally run by
the user directly.  Instead, they are run by Emacs. That is: Emacs
runs the Emacspeak code, which executes Tcl, which interprets the
server code. This approach is too closely ,,wired'' to usage with
Emacspeak, so it can't be used for our general purposes.

This does not mean, that these servers are completely a bad idea and
we can not use them. Thanks to the author Jim Van Zandt
<@email{jrv@@vanzandt.mv.com}>, we can learn from the sources and
write the output driver modules for Speech Dispatcher (emacspeak-ss is
GPL).


@item GTK+ (Gnome Accessibility project)

GNOME windowing toolkit library.

@item wxWindows

Windowing toolkit library.

@item Java AWT

Windowing toolkit library.

@item FOX toolkit

Windowing toolkit library.

@item Speakup

Speakup is a kernel patch that provides low level speech output for visually
impaired, so it works even if there is some problem in configuration and
you can't run Emacspeak.

@item Brltty

Brltty is mainly a driver for different Braille displays, but also supports
some kind of software synthesis.

@end itemize
@end enumerate

We hope to be able to integrate Speech Dispatcher into these projects
in the future.

@node Basic design, User, Current state, Introduction
@section Design
@cindex Design

The communication between all
these applications and synthesizers is a great mess. For this purpose,
we wanted Speech Dispatcher to be a layer separating applications and
synthesizers so that applications wouldn't have to care about synthesizers
and synthesizers wouldn't have to care about interaction with applications.

We decided we would implement Speech Dispatcher as a server receiving
commands from applications over a protocol called @code{SSIP},
parsing them and if it's necessary and calling appropriate functions
of output modules communicating with the different synthesizers.
These output modules are implemented as plug-ins, so that the user
can just load a new module if he wants to use new synthesizer.

Each client (application that wants to speaks) opens a socket
connection to Speech Dispatcher and calls functions like spd_say(),
spd_stop(), spd_pause() provided by the shared library. This
shared library is still on the client side and sends Speech
Dispatcher SSIP commands over the socket. When these arrive
at Speech Dispatcher, it parses them, reads the text that should
be said and put it in a queues according to the priority
of this message and other criteria. It then decides when,
with which parameters (set up by the client and the user)
and on which synthesizer it will say the message. These requests
are handled by the output plug-ins (output modules) for different
hardware and software synthesizers and then said aloud.

See this figure:

@image{figures/architecture,,,Speech Dispatcher architecture (you can see a text
version of this figure in the Info manual)}

See also the detailed description of SSIP, public API and module API.

@node User, Programmer, Basic design, Introduction
@section User's point of view

In this section we will try to describe what can Speech Dispatcher offer
to common users. But every programmer interested in this program should
also read this because it's very important to understand.

@itemize @bullet
@item easy configuration of different speaking applications, central maintenance
@item the ability to freely choose which synthesizer with which application
@item less time devoted to configuration and tuning different applications and synthesis
@item history of said messages for visually impaired
@end itemize

@c TODO: needs a lot of more work

@node Programmer,  , User, Introduction
@section Programmer's point of view

@itemize @bullet
Sketch:
@item easy way to make your applications speak
@item no time spent on configuration/debugging interface with different synthesizers
@item no need to take care about configuration of voice
@item easy way to make the application accessible to visually impaired people
@item different facilities like the one providing a command line functionality
@end itemize


@node Internal structure, Public API, Introduction, Top
@chapter Internal structure

@menu
* Definitions::                 What is output module, who is client...
* Server core::                 Message handling, configuration, history
* Output modules::              How they work and what we need from them
@end menu

@node Definitions, Server core, Internal structure, Internal structure
@section Definitions

@dfn{Server side} is the side where Speech Dispatcher operates. It
means server core, output modules and partly SSIP which is the layer
for communication between server side and client side.

@dfn{Client side} is where particular applications wanting to speak
are, where the shared library implementing public API is
located and partly SSIP which is the layer
for communication between server side and client side.

@dfn{Client} means an application that wants to speak or an application
that is used to control Speech Dispatcher. (Of course different combinations
are possible.)

@dfn{Server core} is the central part of Speech Dispatcher composed of
two threads. One is listening on the user socket, parsing and proceeding
incoming commands, and saving incoming text to queues. The other thread
takes messages from queues and sends them to appropriate synthesizers.

@dfn{Output module} is a backend of Speech Dispatcher in the form of plug-in.
It takes care of communication with the particular synthesizer and provides
only abstract functions to the server core.

@dfn{Shared library} or @dfn{Public API} is a front-end of Speech Dispatcher
that provides polished functions programmers should
use to send commands to the server.

@dfn{SSIP} is the layer (communication protocol) between server side
(server core) and client side (shared library). It stands for Speech
Synthesis Internet Protocol.

@dfn{Socket} or @dfn{File descriptor} represents the particular connection
between a client and server. In C, it's and integer variable.

@node Server core, Output modules, Definitions, Internal structure
@section Server core

see sources, I'll try to write this section soon

@node Output modules,  , Server core, Internal structure
@section Output modules

Output modules for Speech Dispatcher have the form of glib plug-ins
(dynamically loaded libraries) located in src/modules/. They
should have a firm structure, contain all the required functions
and data structures.

@menu
* Basic structure::             The definition of an output module
* How to write new output module::  
* Output module functions::     stop(), pause(), ...   
* Configuration of output modules::  
@end menu

@node Basic structure, How to write new output module, Output modules, Output modules
@subsection Basic structure

The heart of each output module is this structure:

@example
typedef struct@{
    gchar    *name;
    gchar    *description;
    GModule  *gmodule;
    gint     (*write)       (gchar *, size_t, TFDSetElement*);
    gint     (*stop)        (void);
    size_t   (*pause)       (void);
    gint     (*is_speaking) (void);
    gint     (*close)       (void);
    SPDModuleSettings settings;
@}OutputModule;
@end example

Each output module fills in some of the items (e.g. name and description, function
calls pointers) on module_load() and then Speech Dispatcher provides the rest
(gmodule, settings). After module_init(), this structure is fully working and
Speech Dispatcher will communicate with the output module only through
this simple interface. So you can see what are the functions each new
output module must implement, nothing else is necessary.

To be able to load the output module, two additional functions must
be present in it's code:

@example
OutputModule* module_load(configoption_t **options, int *num_options);
int module_init(void);
@end example

They are discussed in more details in @ref{How to write new output module}.

@node How to write new output module, Output module functions, Basic structure, Output modules
@subsection How to write new output module
We will recommend here a basic structure of the code for an output module
you should follow, although it's perfectly ok to establish your own if
you have reasons to do so, if all the necessary functions and data are
defined somewhere in the file. For this purpose, we will use examples
from the output module for Flite (Festival Lite), so it's recommended
to keep looking at @code{flite.c} for reference.

A few rules you should respect:
@itemize
        @item Everything is declared as @code{static} except where explicitly desired to be otherwise.
        @item If one or more new threads are used in the output module, they must block all signals.
        @item On module_close(), all lateral threads and processes should be terminated,
        all memmory freed. Don't suppose module_close() is allways called before exit()
        and the sources will be freed automatically.
        @item All of the functions and macros you define should have prefix module_
        @item We will be happy if all the copyrights are assigned to Brailcom, o.p.s.
        in order for us to be in a better legal position against possible intruders.
@end itemize

The basic structure OutputModule is defined in @file{intl/modules.h}
and therefore  this header must be included in every plug-in source code.

@example
#include "modules.h"
@end example

Also one other file called @file{intl/fdset.h} where the TFDSetElement
structure is defined must be included to be able to handle the
different speech synthesis settings.

@example
#include "fdset.h"
@end example

Additionally, it's also reccomended to include @code{module_utils.c}
which provides many tools to help writing output modules
and make the code simpler.

Definition of macros @code{MODULE_NAME} and @code{MODULE_VERSION}
should follow:

@example
#define MODULE_NAME     "flite"
#define MODULE_VERSION  "0.1"
@end example

If you want to use the @code{DBG(message)} macro from @code{module_utils.c}
to print out debugging messages, you should insert these two lines. (Please
don't use printf for debugging, this doesn't work with threads!)
(You will later have to actually start debugging in @code{module_load()})

@example
#define DEBUG_MODULE 0  /* Switch to 1 for debugging */
DECLARE_DEBUG_FILE("/tmp/debug-flite");
@end example

You also have to define the function prototypes, first for your
own functions you use in your module internals, and then also
the public functions which are passed to the OutputModule
structure. You can do this easily by using the macro
@code{DECLARE_MODULE_PROTOTYPES()} (See module_utils.c
for the expansion.)

@example
/* Public function prototypes */
DECLARE_MODULE_PROTOTYPES();
@end example

When these prototypes are defined, you can finally define
the OutputModule structure for your plugin, again, it can
be easily done using a macro (recommended: see module_utils.c
for the expansion).

@example
DECLARE_MODINFO("flite", "Flite software synthesizer v. 1.2");
@end example

You should specify the name of your output module, which should
coincide with the filename of that plugin, in the first
parameter and a short description in the second one, together
with the version of the @emph{synthesizer} this plugin is
intended for. The version of your @emph{plugin} will be appended
automatically from MODULE_VERSION.

Optionally, if your output module requires some special configuration,
apart from defining voices (they are handled differently, see bellow),
you can declare the requested option here. It will expand into a dotconf
callback and declaration of the variable.

(You will later have to actually register these options for
Speech Dispatcher in @code{module_load()})

There are currently 4 types of possible configuration options:
@itemize
@item @code{MOD_OPTION_1_INT(name);   /* Set up `int name' */}
@item @code{MOD_OPTION_1_STR(name);   /* Set up `char* name' */}
@item @code{MOD_OPTION_2(name);       /* Set up `char *name[2]' */}
@item @code{MOD_OPTION_@{2,3@}_HT(name);  /* Set up a hash table */}
@end itemize

@xref{Configuration of output modules}.

For example Flite uses 2 options:
@example
MOD_OPTION_1_INT(FliteMaxChunkLenght);
MOD_OPTION_1_STR(FliteDelimiters);
@end example

This is the end of the declarations section, take a deep breath, function
definitions follow.

Every output module is being started in 2 phases: @emph{loading} and @emph{initialization}.
The goal of loading is to pass Speech Dispatcher a pointer to OutputModule,
init debugging and register the configuration options. In the later phase,
initialization, all the configuration has been read yet and the output module
can accomplish the rest (check if the synthesizer works, set up threads etc.).

You should start by the definition of @code{module_load()}. Note that
this function @emph{can't} be static in order to be able to load the
module!

@example
OutputModule* module_load(configoption_t **options, int *num_options)@{
@end example

First of all, you should start debugging, if you decided to use 
the DBG() macro. @code{module_utils.c} offers this macro:

@example
    INIT_DEBUG_FILE();
@end example

So, lets finally register the configuration options. If you
use the @code{module_utils.c} macros, you don't have to
touch the parameters @code{options} and @code{num_options}
directly. If you don't, please see @code{module_utils.c}
and DotConf documentation for more details.

Just use these macros:
@itemize
        @item MOD_OPTION_1_INT_REG(name, default);  /* for integer parameters */
        @item MOD_OPTION_1_STR_REG(name, default);  /* for string parameters */
        @item MOD_OPTION_MORE_REG(name);   /* for an array of strings */
        @item MOD_OPTION_HT_REG(name);     /* for hash tables */
@end itemize

Again, example from Flite:
@example
    MOD_OPTION_1_INT_REG(FliteMaxChunkLenght, 300);
    MOD_OPTION_1_STR_REG(FliteDelimiters, ".");
@end example

If you want to enable the mechanism for setting
voices through AddVoice, use this function (for
an example see @code{festival.c}):

@example
    module_register_settings_voices(&module_info);
@end example

@xref{Configuration of output modules}.

At this place, before terminating, Flite prints it's first debugging message:

@example
    DBG("module_load()\n");
@end example

The last thing you have to do here is return the module's OutputModule
structure.

@example
    return &module_info;
@}
@end example

The second phase of starting an output module is handled by
(also @emph{non-static}):

@example
int module_init(void)@{
[...]
@}
@end example

The body of this function is entirely up to you. You should do all
the necessary initialization of the particular synthesizer.
All declared configuration variables and configuration hash
tables, together with the definition of voices, are filled
with their values (either default or read from configuration),
so you can use them yet.

This function should return 0 if the module was initialized
successfully, or -1 if some failure was encountered. In
this case, you should clean up everything, cancel threads,
deallocate memory etc, no more functions of this output
module will be touched (except for other tries to load
and initialize the module).

Example from Flite:
@example
    /* Init flite and register a new voice */
    flite_init();
    flite_voice = register_cmu_us_kal();

    if (flite_voice == NULL)@{
        DBG("Couldn't register the basic kal voice.\n");
        return -1;
    @}
    [...]
@end example

Now you have to define the synthesis control functions @code{module_write}, @code{module_stop}
etc. @ref{Output module functions}, and try to implement it somehow. If it doesn't work
(and it will probably not), it's most likely not your fault, complain! This manual
is not complete and the instructions in this sections aren't either. Get in touch with
us and together we can figure out what's wrong, fix it and then warn others in this
manual.

@node Output module functions, Configuration of output modules, How to write new output module, Output modules
@subsection Output module functions

Note that all these functuions have to be declared as static,
so that they don't mix with other output modules.

@deffn {Output module functions} static gint module_write (const gchar *data, size_t bytes, TFDSetElement* set)
@findex module_write()

This is the function where actual speech output is produced. It is called
every time Speech Dispatcher decides to send a message to synthesis. The data
of length @var{len} are passed in @var{data}. Additionally, the structure
containing settings associated to this particular message is passed,
although only few options are important for output modules.

Each output module should take care of setting the output device to these parameters
(the other ones are handled independently in other parts of Speech Dispatcher):
@itemize @bullet
@item (signed int) set->speed
@item (signed int) set->pitch
@item (char*) set->language
@item (int) set->voice_type
@end itemize

Speed and pitch are values between -100 and 100 included. 0 is the default
value that represents normal speech flow. So -100 is the slowest (or lowest)
and +100 is the fastest (or highest) speech.

The language parameter is given as a null-terminated string containing 
the name of the language according to RFC 1776 (en, cs, fr, ...). If the
requested language is not supported by this synthesizer, it's ok to abort
and return 0, because that's an error in user settings.

voice_type is used only when the output module supports more types of voices
for this particular language. The values represent (from @file{intl/fdset.h})
@example
typedef enum @{
    NO_VOICE = 0,
    MALE1 = 1,
    MALE2 = 2,
    MALE3 = 3,
    FEMALE1 = 4,
    FEMALE2 = 5,
    FEMALE3 = 6,
    CHILD_MALE = 7,
    CHILD_FEMALE = 8
@}EVoiceType;
@end example
We can consider also other voice types. 

This function should return 0 if it fails and 1 if the delivery
to the synthesis is successful. It should return immediately,
because otherwise, it would block stopping, priority handling
and other important things in Speech Dispatcher.

If there is a need to stay longer, you
should create a separate thread or process. This is for example
the case of some software synthesizers who use a blocking
function (eg. spd_audio_play) or hardware devices that have
to send data to output modules at some particular speed. Note
that if you use threads for that purpose, you have to set them
to ignore all signals. See @code{flite.c} for reference.

@end deffn

@deffn {Output module function}  {static gint module_stop} (void)
@findex synthesizer_stop()

This function should stop the synthesis of the currently spoken message
immediately and throw away the rest of the message.

It should return 0 on success, -1 otherwise.

@end deffn

@deffn {Output module function}  {static size_t synthesizer_pause} (void)
@findex module_pause()

This function should stop speaking on the synthesizer (or sending
data to soundcard) and return the position where the text was
interrupted.

This may be a different approach of what you would
expect on the first sight, but it's important to understand that
simply sending some @@pause to the output device won't work.
In Speech Dispatcher, these requests for pause are handled on a
client-per-client basis, so the output device can't be blocked
for other clients. For example, a user can pause the text in
Emacs buffer in order to, say, check new mail. If the output
device would be blocked by the Emacs pause, he couldn't check
his email, because there would be no output.

It's ok not to return the exact text position in precission of letters,
you can for example take sentences as units. Also, this function
can be blocking for a short time, so you can wait till the last
sentence is said and then return the text from the end of this
sentence. 

For some software synthesizer the desired effect can be archieved in this way:
When @code{module_write()} is called, you execute a separate
process and pass it the requested message. This process
cuts the message into sentences and then runs in a loop
and sends these pieces to synthesis. If a signal arrives
from @code{module_pause()}, you set a flag and stop that loop
at the point where next piece of text would be synthesized.
Instead, you return the rest of the unprocessed text to
@code{module_pause()}.

It's generally better if the ``returned text'' overlays
a little bit with what was actually said than if there is
a gap.

If the given output device doesn't support synchronization
(@pxref{synthesizer_is_speaking}), it should execute STOP on the output
device and return 0.

It's not an error if this function is called when the device
is not speaking. In this case, it should return 0.

@end deffn

@deffn {Output module function}  {static gint module_is_speaking} (void)
@findex module_is_speaking()
@anchor{synthesizer_is_speaking}

This function is very important to let Speech Dispatcher know how to
regulate the speech flow between different queues, programs and even
other synthesizers. On calling it, the output module must decide
whether there is currently any output being produced in the speakers.

It should return 0 if the synthesis is silent, 1 if it is speaking.

This can be a very hard problem and it's not clear how to do it
with different synthesizers that don't support backward communication.
Sometimes maybe there is a possibility to calculate a good estimate
(one second or so), but if there is really no way how to tell,
it can also return 2 for ``don't know''. The usefulness of such
output module would be highly reduced: there will be problems in
cooperating with other output devices, no real possibility to
use sound-mapped sound icons, priorities wouldn't sometimes work,
it will sometimes cut-off text etc.

You should do your best to get it working (sometimes there are
not-obvious ways around), but if this effort fails, it's better
to return 2 than a bad estimate (which can cause yet a greater damage).
Not that such a module is still useful, because the user can map all
his sound-icons to text, use only this output device etc.

This function should return very fast, because Speech Dispatcher calls
it very often for different purposes.

@end deffn

@deffn {Output module function}  {static gint module_close} (void)
@findex module_close()

This function is called when Speech Dispatcher terminates. The output
module should terminate all threads and processes, free all resources,
close all sockets etc. Never assume this function is called only when
Speech Dispatcher terminates and exit(0) will do the work for you. It's
perfectly ok for Speech Dispatcher to load, unload or reload output modules
in the middle of it's run.

It should return 0 on success, -1 otherwise.

@end deffn

@node Configuration of output modules,  , Output module functions, Output modules
@subsection Configuration of output modules

@node Public API, SSIP, Internal structure, Top
@chapter Public API

@menu
* SSIP (socket communication)::  Direct connection
* C API::                       Shared library for C/C++
* Python API::                  
@end menu

@node SSIP (socket communication), C API, Public API, Public API
@section SSIP

In general, it's much better to use one of the shared
libraries for specific languages. However, there can
be reasons to communicate with Speech Dispatcher directly.

In this case, please see @ref{SSIP}.

@node C API, Python API, SSIP (socket communication), Public API
@section C API

TODO: introduction

All functions, except where explicitly stated, aren't blocking.

@menu
* Initializing and terminating (C)::  Starting and terminating a session
* Speech synthesis commands (C)::  Sending text to synthesis
* Speech output control commands (C)::  Stopping, pausing, resuming
* Characters and keys (C)::     
* Sound icons (C)::             Icons mapped to synthesis or sound files         
* Parameter setting commands (C)::  Voice configuration, different modes...
* Other functions (C)::         
* Information retrieval commands (C)::  Getting information from server
* History commands (C)::        History of previously queued messages
@end menu

@node Initializing and terminating (C), Speech synthesis commands (C), C API, C API
@subsection Initializing and terminating (C)

@deffn {C API function}  int spd_open(char* client_name, char* connection_name, char* user_name)
@findex spd_open()

Opens a new connection to Speech Dispatcher and returns a socket file descriptor you will
use to communicate with Speech Dispatcher. It's one of the parameter of all the others
functions.

The three parameters @code{client_name}, @code{connection_name} and @code{username}
are there only for informational and navigational purpose, they don't affect
any settings or behavior of any functions. The authentication mechanism
has nothing to do with @code{username}. These parameters are important for
the user when he wants to set some parameters for a given session, when he
wants to browse through history, etc.

@code{client_name} is the name of the client that opens the connection. Normally,
it should be the name of the executable, for example ``lynx'', ``emacs'', ``bash'',
or ``gcc''. It can't be left to NULL.

@code{connection_name} determines the particular use of that connection. If you
use only one connection in your program, this should be set to ``main'' (passing
a NULL pointer has the same effect). If you use two or more connections in
your program, their @code{client_name}s should be the same, but @code{connection_name}s
should differ. For example: ``buffer'', ``command_line'', ``text'', ``menu''.

@code{username} should be set to the name of the user. Normally, you should
get this string from the system. If set to NULL, libspeechd will try to
determine it automatically by g_get_user_name().

It returns the file descriptor of the created connection on success, 0 on error.

Each connection you open should be closed by spd_close() before the end of the program.

@end deffn

@deffn {C API function}  void spd_close(int connection)
@findex spd_close()

Closes a Speech Dispatcher socket connection. You should close every connection
before the end of your program.

@code{connection} is the file descriptor obtained by spd_open().
@end deffn

@node Speech synthesis commands (C), Speech output control commands (C), Initializing and terminating (C), C API
@subsection Speech synthesis commands (C)

@deffn {C API function}  int spd_say(int connection, char* priority, char* text);
@findex spd_say()

Sends a message to Speech Dispatcher. If this message isn't blocked by some message
of higher priority and this CONNECTION isn't paused, it will be synthesized
directly on some of the output devices. Otherwise, the message will be discarded
or delayed according to it's priority.

@code{connection} is the file descriptor obtained by spd_open().

@code{priority} is a NULL terminated string with the name
of the priority. @xref{Priorities}.

@code{text} is a null terminated string containing text you want to sent to
synthesis. It must be encoded in UTF-8. Note that this doesn't have to be what
you will finally hear. It can be affected by different settings as spelling,
punctuation, text substitution etc.

It returns 0 on success, -1 otherwise.

@end deffn

@deffn {C API function}  int spd_sayf(int connection, char* priority, char* format, ...);
@findex spd_sayf()

Similar to @code{spd_say()}, simulates the behavior of printf().

@code{format} is a string containing text and format of the parameters, like ``%d'',
``%s'' etc. It must be encoded in UTF-8.

@code{...} is an arbitrary number of arguments.

All other parameters are the same as for spd_say().

For example:
@example
       spd_sayf(conn, "text", ``Hello %s, how are you?'', username);
       spd_sayf(conn, "important", ``Fatal error on [%s:%d]'', filename, line);
@end example

But be careful! For example this doesn't work:

@example
       spd_sayf(conn, "notification", ``Pressed key is %c.'', key);
@end example

Why? Because you are supposing that key is a char, but that will
fail with languages using amplified charsets. The proper solution
is:

@example
       spd_sayf(conn, "notification", ``Pressed key is %s'', key);
@end example
where key is a encode string.

It returns 0 on succes, -1 otherwise.
@end deffn

@node Speech output control commands (C), Characters and keys (C), Speech synthesis commands (C), C API
@subsection Speech output control commands (C)

@subsubsection Stop commands

@deffn {C API function}  int spd_stop(int connection);
@findex spd_stop()

Stops the message currently being spoken on a given connection. If there
is no message being spoken, does nothing. (It doesn't touch the messages
waiting in queues). This is intended for stops executed by the user,
not for automatic stops (because automatically, you can't control
how many messages are still waiting in queues on the server).

@code{connection} is the file descriptor obtained by spd_open().

It returns 0 on succes, -1 otherwise.

@end deffn

@deffn {C API function}  int spd_stop_all(int connection);
@findex spd_stop_all()

The same as spd_stop(), but it stops every message being said,
without distinguishing where it came from.

It returns 0 on succes, -1 if some of the stops failed.

@end deffn

@deffn {C API function}  int spd_stop_uid(int connection, int target_uid);
@findex spd_stop_uid()

The same as spd_stop() except that it stops some other client than
the calling one. You must specify this client in @code{target_uid}.

@code{target_uid} is the unique ID of the connection you want
to execute stop() on. It can be obtained from spd_history_get_client_list(),
@xref{History commands (C)}.

It returns 0 on succes, -1 otherwise.

@end deffn

@subsubsection Cancel commands

@deffn {C API function}  int spd_cancel(int connection);

Stops the currently spoken message from this connection
(if there is any) and discard all the queued messages
from this connection. This is probably what you want
to do, when you call spd_cancel() automatically in
your program.

@end deffn

@deffn {C API function}  int spd_cancel_all(int connection);
@findex spd_cancel_all()

The same as spd_cancel(), but it cancels every message
without distinguishing where it came from.

It returns 0 on succes, -1 if some of the stops failed.

@end deffn

@deffn {C API function}  int spd_cancel_uid(int connection, int target_uid);
@findex spd_cancel_uid()

The same as spd_cancel() except that it executes cancel for some other client
than the calling one. You must specify this client in @code{target_uid}.

@code{target_uid} is the unique ID of the connection you want
to execute cancel() on. It can be obtained from spd_history_get_client_list(),
@xref{History commands (C)}.

It returns 0 on succes, -1 otherwise.

@end deffn

@subsubsection Pause commands

@deffn {C API function}  int spd_pause(int connection);
@findex int spd_pause()

Pauses all messages received from the given connection. No messages
except for priority @code{notification} and @code{progress} all threw away,
they are all waiting in
a separate queue for resume(). The message that was being said
in the moment pause() was received will be continued from the place
where it was paused.

To be able to guarantee this, we had to define this function
as blocking. On some devices, it can return immediately
while on some others, it can return after a short period
of time, for example after some sentence is synthesized till
the end.

This shouldn't be any problem. User executes a pause, he waits
a short time till the end of the sentence, than he can do what
he wants and when he return, resume() is sent and Speech Dispatcher
will continue saying the message without any loss of information.

Never suppose this function doesn't block, even if on your
output device it doesn't.

When pause is on for the given client, all newly received
messages are also queued and waiting for resume().

It returns 0 on succes, -1 if something failed.

@end deffn

@deffn {C API function}  int spd_pause_all(int connection);
@findex spd_pause_all()

The same as spd_pause(), but it pauses every message,
without distinguishing where it came from.

It returns 0 on succes, -1 if some of the pauses failed.

@end deffn

@deffn {C API function}  int spd_pause_uid(int connection, int target_uid);
@findex spd_pause_uid()

The same as spd_pause() except that it executes pause for some other client
than the calling one. You must specify this client in @code{target_uid}.

@code{target_uid} is the unique ID of the connection you want
to pause. It can be obtained from spd_history_get_client_list(),
@xref{History commands (C)}.

It returns 0 on succes, -1 otherwise.

@end deffn

@subsubsection Resume commands

@deffn {C API function}  int spd_resume(int connection);
@findex int spd_resume()

Resumes all paused messages from the given connection. The rest
of the message that was being said in the moment pause() was
received will be said and all the other messages are queued
for synthesis again.

@code{connection} is the file descriptor obtained by spd_open().

It returns 0 on succes, -1 otherwise.

@end deffn

@deffn {C API function}  int spd_resume_all(int connection);
@findex spd_resume_all()

The same as spd_resume(), but it resumes every paused message,
without distinguishing where it came from.

It returns 0 on succes, -1 if some of the pauses failed.

@end deffn

@deffn {C API function}  int spd_resume_uid(int connection, int target_uid);
@findex spd_resume_uid()

The same as spd_resume() except that it executes resume for some other client
than the calling one. You must specify this client in @code{target_uid}.

@code{target_uid} is the unique ID of the connection you want
to resume. It can be obtained from spd_history_get_client_list(),
@xref{History commands (C)}.

It returns 0 on succes, -1 otherwise.
@end deffn

@node Characters and keys (C), Sound icons (C), Speech output control commands (C), C API
@subsection Characters and keys

@deffn {C API function}  int spd_char(int connection, int priority, char* character);
@findex spd_char()

Says a character according to user settings for characters. This can be
used for example for reading letters under the cursor.

@code{connection} is the file descriptor obtained by spd_open().

@code{priority} is a NULL terminated string with the name
of the priority. @xref{Priorities}.

@code{character} is a NULL terminated string of chars containing one UTF-8
character. If it contains more characters, only the first one is processed.

It returns 0 on succes, -1 otherwise.

@end deffn

@deffn {C API function}  int spd_wchar(int connection, int priority, wchar_t wcharacter);
@findex spd_say_wchar()

The same as spd_char(), but it takes a wchar_t variable as it's argument. 

It returns 0 on succes, -1 otherwise.
@end deffn

@deffn {C API function}  int spd_key(int connection, char* key_name);
@findex spd_key()

Says a key according to user settings for keys.

@code{connection} is the file descriptor obtained by spd_open().

@code{priority} is a NULL terminated string with the name
of the priority. @xref{Priorities}.

@code{key_name} is the name of the key in a special format.
@xref{Speech synthesis and sound output commands}, (KEY, the associated
SSIP command) for description of the format of @code{key_name}

It returns 0 on succes, -1 otherwise.
@end deffn

@node Sound icons (C), Parameter setting commands (C), Characters and keys (C), C API
@subsection Sound icons

@deffn {C API function}  int spd_sound_icon(int connection, int priority, char* icon_name);
@findex spd_sound_icon()

Sends a sound icon ICON_NAME. These are symbolic names that are mapped
to a sound or to a text (in the particular language) according to
Speech Dispatcher tables and user settings. Each program can also
define it's own icons.

@code{connection} is the file descriptor obtained by spd_open().

@code{priority} is a NULL terminated string with the name
of the priority. @xref{Priorities}.

@code{icon_name} is the name of the icon. It can't contain spaces,
please use underscores (`_').

@end deffn

@node Parameter setting commands (C), Other functions (C), Sound icons (C), C API
@subsection Parameter settings commands

The following parameter setting commands are available. For configuration
and history clients there are also functions for setting the value for
some other connection and for all connections. They are listed bellow separately.

Please @xref{Speech parameters}, for a general description of what they mean.

@deffn {C API function}  int spd_set_language(int connection, char* language);
@findex spd_set_language()
@end deffn

Sets the language that should be used for synthesis, selecting tables
and other related things.

@code{connection} is the file descriptor obtained by spd_open().

@code{language} is the language code as defined in RFC 1776 (``cs'',
``en'', ...).

@deffn {C API function}  int spd_set_punctuation(int connection, SPDPunctuation type);
@findex spd_set_punctuation()

Set punctuation mode to the given value.  `all' means read all
punctuation characters, `none' read no punctuation characters,
`some' means read only punctuation characters given in the server
configuration or defined by the client's last spd_set_punctuation_important().

@code{connection} is the file descriptor obtained by spd_open().

@code{type} is one of the following values: @code{SPD_PUNCT_ALL},
@code{SPD_PUNCT_NONE}, @code{SPD_PUNCT_SOME}.

It returns 0 on succes, -1 otherwise.

@end deffn

@deffn {C API function}  int spd_set_punctuation_important(int connection, char* punctuation_important);
@findex spd_set_punctuation_important()

Set punctuation characters that are pronounced if punctuation
is set to @code{some}. It can be for example @code{!?.;}.

@code{connection} is the file descriptor obtained by spd_open().

@code{punctuation_important} is a NULL terminated string containing
the chosen characters. Space is also an allowed character so don't put them
in if this is not what you want.

It returns 0 on succes, -1 otherwise.

@end deffn

@deffn {C API function}  int spd_set_spelling(int connection, SPDSpelling type);
@findex spd_set_spelling()

Switches spelling mode on and off. If set to on, all incomming messages
from this particular connection will be processed according to appropriate
spelling tables (see spd_set_spelling_table()).

@code{connection} is the file descriptor obtained by spd_open().

@code{type} is one of the following values: @code{SPD_SPELL_ON}, @code{SPD_SPELL_OFF}.

@end deffn

@deffn {C API function}  int spd_set_spelling_table(int connection, char* spelling_table);
@findex spd_set_spelling_table()
@end deffn
@deffn {C API function}  int spd_set_punctuation_table(int connection, char* punctuation_table);
@findex spd_set_punctuation_table()
@end deffn
@deffn {C API function}  int spd_set_text_table(int connection, char* text_table);
@findex spd_set_text_table()
@end deffn
@deffn {C API function}  int spd_set_sound_table(int connection, char* sound_table);
@findex spd_set_sound_table()
@end deffn
@deffn {C API function}  int spd_set_character_table(int connection, char* character_table);
@findex spd_set_character_table()
@end deffn
@deffn {C API function}  int spd_set_key_table(int connection, char* key_table);
@findex spd_set_key_table()
@end deffn

Sets the appropriate table for spelling, punctuation, text substitution,
sound icons, character reading and key reading respectively. If you
are going to do this, you must either get the names of the available
tables by the corresponding spd_list_[type]_tables() and let the user
choose or use one of the standard tables @xref{Standard spelling tables},
@xref{Standard sound tables}.

@code{connection} is the file descriptor obtained by spd_open().

@code{type_table} is a NULL terminated string containing the name of
the requested table.

@deffn {C API function}  int spd_set_voice_type(int connection, SPDVoiceType voice);
@findex spd_set_voice()
@end deffn

@deffn {C API function}  int spd_set_voice_rate(int connection, int rate);
@findex spd_set_rate()

Set voice rate.

@code{connection} is the file descriptor obtained by spd_open().

@code{rate} is a number between -100 and +100 which means
the slowest and the fastest speech rate respectively.

@end deffn

@deffn {C API function}  int spd_set_voice_pitch(int connection, int pitch);
@findex spd_set_pitch()

Set voice pitch.

@code{connection} is the file descriptor obtained by spd_open().

@code{pitch} is a number between -100 and +100, which means the
lowest and the highest pitch respectively.

@end deffn

@deffn {C API function} int spd_set_punctuation_table(int connection, const char* ptable);
@findex spd_set_punctuation_table()
@end deffn
@deffn {C API function} int spd_set_spelling_table(int connection, const char* stable);
@findex spd_set_spelling_table()
@end deffn
@deffn {C API function} int spd_set_text_table(int connection, const char* ttable);
@findex spd_set_text_table()
@end deffn
@deffn {C API function} int spd_set_sound_table(int connection, const char* stable);
@findex spd_set_sound_table()
@end deffn
@deffn {C API function} int spd_set_char_table(int connection, const char* ctable);
@findex spd_set_char_table()
@end deffn
@deffn {C API function} int spd_set_key_table(int connection, const char* ktable);
@findex spd_set_key_table()
@end deffn


@deffn {C API function}  int spd_set_history(int connection, int history_flag);
@findex spd_set_history()

This function should never be used except by history clients when they are saying
message that already is in history!

@end deffn

@node Other functions (C), Information retrieval commands (C), Parameter setting commands (C), C API
@subsection Other functions
@findex spd_command_line()

@node Information retrieval commands (C), History commands (C), Other functions (C), C API
@subsection Information retrieval commands

@node History commands (C),  , Information retrieval commands (C), C API
@subsection History commands
@findex spd_history_select_client()
@findex spd_get_client_list()
@findex spd_get_message_list_fd()

libspeechd.c currently doesn't support history commands as defined in SSIP

@node Python API,  , C API, Public API
@section Python API

@c You should name all subnodes with a (Python) at the
@c end or there will be conflicts with other APIs
@c There is no point in doing the same for @chapters
@c and sections.
@c See the C API...

@node SSIP, Priorities, Public API, Top
@chapter Speech Synthesis Internet Protocol (SSIP)

Clients communicate with Speech Dispatcher via the Speech Synthesis
Internet Protocol (SSIP).  The protocol is the actual interface to
Speech Dispatcher.

Usually, you don't need to use SSIP directly, you can use one of the
programming interfaces, see @ref{Public API}, wrapping SSIP with
programming library calls.  This is a recommended way of communication
with Speech Dispatcher.  However, in case your programming environment is
not supported by any of the provided interfaces or you prefer to
communicate with Speech Dispatcher directly for any reason, you can find
the complete SSIP description here.

@menu
* General rules::               Overall conventions applying to SSIP.
* SSIP commands::               Complete reference of SSIP commands.
* Return codes::                List of SSIP result codes.
* Sample SSIP relation::        An example session.
@end menu

@node General rules, SSIP commands, SSIP, SSIP
@section General rules

SSIP communicates with the clients through a defined set of text
commands, in the way usual in common Internet protocols.  The
characters sent to and from Speech Dispatcher are encoded using the UTF-8
encoding.

Each SSIP command, unless specified otherwise, consists of exactly one
line.  The line is sent in the following format:

@example
@var{command} @var{arg} ...
@end example

where @var{command} is a case insensitive command name and @var{arg}s
are its arguments separated by spaces.  The command arguments which
come from a defined set of values are case insensitive as well.  The
number of arguments is dependent on the particular command and there
can be commands having no arguments.

All lines of SSIP input and output must be ended with the pair of
carriage return and line feed characters, in this order.

When you connect to Speech Dispatcher, you should at least set your client
name, through the @code{SET self CLIENT_NAME} command, @ref{Parameter
setting commands}.  This is important to get a proper identification
of your client --- to allow managing it from the control center
application and to identify it in a message history browser.  You
might want to set other connection parameters as well, look for more
details in @ref{Parameter setting commands}.

Connection to Speech Dispatcher is preferably closed by issuing the
@code{QUIT} command, see @ref{Other commands}.

SSIP is a synchronous protocol --- you send commands and only after a
complete response from SSIP arrives back you are allowed to send the
next command.  Usually, the connection to Speech Dispatcher remains open
during the whole run of the particular client application.  If you
close the connection and open it again, you must set all the
previously set parameters again, Speech Dispatcher doesn't store session
parameters between connections.

The protocol allows you to perform commands regarding other currently
connected or previously connected clients.  This allows you to write a
control application managing or browsing all the messages received by
the current Speech Dispatcher process.  The mechanism is completely
relaxed, there are no restrictions on accessing messages of other
clients and users and managing some aspects of their sound output.

SSIP replies of Speech Dispatcher are of the following format:

@example
@var{ccc}-line 1
@var{ccc}-line 2
...
@var{ccc}-line @var{n}-1
@var{ddd} line @var{n}
@end example

where @var{n} is a positive integer, and @var{ccc} and @var{ddd} are
three-digit long numeric codes identifying the result of the command.
The last line determines the overall result of the command, the result
code is followed by an English message describing the result of the
action in a human readable form.

@node SSIP commands, Return codes, General rules, SSIP
@section SSIP commands

Commands recognized by SSIP can be divided into several groups: Speech
synthesis and sound output commands, speech control commands,
parameter setting commands, commands retrieving information about
current client and server settings, command handling the message
history, and other commands.  Each of these command groups is
described in one of the following sections.

In the command descriptions, the command is written together with its
arguments.  Optional arguments are enclosed by square brackets
(@code{[} and @code{]}), alternatives are separated by the vertical
rule (@code{|}) and are grouped within braces (@code{@{} and
@code{@}}) or square brackets for mandatory or optional arguments
respectively, literal arguments values are typeset in lower letters
(they are case insensitive), and variable arguments are typeset
@var{like this}.  Ellipsis denoted by three dots (@code{...}) means
repetition (zero or more times) of all the arguments within the
current brackets.

@menu
* Speech synthesis and sound output commands::  
* Speech output control commands::  
* Parameter setting commands::  
* Information retrieval commands::  
* History handling commands::   
* Other commands::              
@end menu

@node Speech synthesis and sound output commands, Speech output control commands, SSIP commands, SSIP commands
@subsection Speech synthesis and sound output

These commands invoke actual output to particular output device.  The
particular way how the message is handled depands on current speech
parameter settings and Speech Dispatcher configuration (see
@pxref{Configuration}).

@table @code
@item SPEAK
Start receiving a text message and synthesize it.  After sending a
reply to the command, Speech Dispatcher waits for the text of the
message.  The text can spread over any number of lines and is
finished by an end of line marker followed by the line containing the
single character @code{.} (dot).  Thus the complete character sequence
closing the input text is @code{CR LF . CR LF}.  If any line within
the sent text starts with a dot, an extra dot is prepended before it.

During reception of the text message, Speech Dispatcher doesn't send
response to the particular lines sent.  The response line is sent only
immediately after the @code{SPEAK} command and after receiving the
closing dot line.

Speech Dispatcher can start speech synthesis as soon as a sufficient
amount of the text arrives, it generally needn't (but may) wait until
the end of data marker is received.

There is no explicit upper limit on the size of the text, but the
server administrator may set one in the configuration or the limit can
be enforced by available system resources.  If the limit is exceeded,
the whole text is accepted, but its exceeding part is ignored and an
error response code is returned after processing the final dot line.

@item CHAR @var{char}
Speak letter @var{char}.  @var{char} can be any character
representable by the UTF-8 encoding. The only exception
is the character space (@code{ }) that can't be sent
directly. In this case, a string @code{space} must be
sent instead.

@example
CHAR e
CHAR \
CHAR space
CHAR &
@end example

This command is intended to be used for speaking single letters,
e.g.@ when reading a character under cursor or when spelling words.

@item KEY @var{key-name}
Speak key identified by @var{key-name}.  The command is intended to be
used for speaking keys pressed by the user.

@var{key-name} is a case sensitive symbolic key name.  It is composed
of a key name, optionally prepended with one or more prefixes, each
containing an auxiliary key name and the underscore character.

Key name may contain any character excluding control characters (the
characters in the range 0 to 31 in the ASCII table, characters in the
range 128 to 159 in the Latin-* tables and other ``invisible''
characters), spaces, underscores, and double quotes.

The recognized key names are:

@itemize
@item
Any single UTF-8 character, excluding the exceptions defined above.

@item
Any of the symbolic key names defined in @ref{Key names}.
@end itemize

Examples of valid key names:

@example
a
A
shift_a
shift_A
@'{u}
$
enter
shift_kp-enter
control_alt_delete
control
@end example

@item SOUND_ICON @var{icon-name}
Send a sound identified by @var{icon-name} to the audio output.
@var{icon-name} is a symbolic name of the given sound from the
standard set listed in @ref{Standard sound icons}, or another name
from the particular Speech Dispatcher sound icon configuration.
@end table

@node Speech output control commands, Parameter setting commands, Speech synthesis and sound output commands, SSIP commands
@subsection Controlling speech output

These commands can stop or resume speech or audio output.  They all
affect only the synthesis process and output to a sound device, they
do not affect the message history.

@table @code
@item STOP @{ @var{id} | all | self @}
Immediately stop outputting the current message (whatever it is ---
text, letter, key, or sound icon) from the identified client, if any
is being output.  If the command argument is @code{self}, last message
from the current client connection is stopped.  If it is @code{all},
stop currently output message or messages from all the clients.
Otherwise, argument @var{id} must be given as an positive integer and
the currently processed message from the client connection identified
by @var{id} is stopped; if there is none such, do nothing.

@item CANCEL @{ @var{id} | all | self @}
This command is the same as @code{SPEAK}, with the exception that it
stops not yet output messages as well.  All currently queued messages
are stored into the message history without being sent to the audio
output device.

@item PAUSE @{ @var{id} | all | self @}
Stop audio output immediately, but do not discard anything.  All the
currently output and currently or later queued messages are postponed
and saved for later processing, until the corresponding @code{RESUME}
command is received.

The meaning of the command arguments is the same as in the @code{STOP}
command.

@item RESUME @{ @var{id} | all | self @}
Cancel the effect of the previously issued @code{PAUSE} command.
Note that messages of the priority ``progress'' and ``notification'' received during
the pause are not output (but they remain stored in the message history).

It is an error to send the @code{RESUME} command when the output
corresponding to the given argument is not paused by a previous
invocation of the @code{PAUSE} command.  Such an error is signaled by
a @code{4XX} return code.

The meaning of the command arguments is the same as in the @code{STOP}
command.
@end table

@node Parameter setting commands, Information retrieval commands, Speech output control commands, SSIP commands
@subsection Parameter setting

The @code{SET} command sets various control parameters of Speech
Dispatcher.  The parameter is always denoted by the first command
argument.

All the settings take effect to the client connections defined by the
first command argument and until the parameter setting is changed by
another invocation of the appropriate @code{SET} command or until the
connection is closed.

The first argument specifying the connection to apply the setting to
may get one of the following values:

@table @code
@item self
The setting applies to the current connection only.
@item all
The setting applies to all open connections.
@item @var{id}
The setting applies to the connection identified by the number @var{id}.
@end table

Not all parameter setting commands may receive all kinds of the first
parameter defined above, for instance, some of them may receive only
@code{self}.

@table @code
@item SET self CLIENT_NAME @var{user}:@var{client}:@var{component}
Set client's name.  Client name consists of the user name, client
(application) identification, and the identification of the component
of the client (application).  Each of the parts of the client name may
contain only alphanumeric characters.

For example, for a client called @code{lynx} that creates Speech
Dispatcher connection for its command processing, the name could be
set in the following way:

@example
SET self CLIENT_NAME joe:lynx:cmd_processing
@end example

The client name is used in the server configuration settings, client
listings and message history handling.  All its three parts can be
arbitrary, but it's important to define and follow rules for each
application supporting Speech Dispatcher, so that a Speech Dispatcher user can
configure all the aspects of the speech output easily.

Usually, this command should be sent as the very first command when a
new connection to Speech Dispatcher is established.  The command may be
sent only once within a single connection, attempts to change the
client's name once it's already set are answered with an error code.

@item SET @{ all | self | @var{id} @} LANGUAGE @var{language}
Set recommended language for this client to @var{language}.
@var{language} is the name of the language according to RFC 1766.

For example, to set the preferred language to Czech, you send the
following command:

@example
SET self LANGUAGE cs
@end example

@item SET self PRIORITY @var{n}
Set message priority to @var{n}.  @var{n} must be one of the values
@code{important}, @code{text}, @code{message}, @code{notification},
@code{progress}.  See @ref{Priorities} for more information.

@item SET @{ all | self | @var{id} @} VOICE @var{name}
Switch to the voice identified by @var{name}.  @var{name} must be one
of the voice identifiers returned by the command @code{LIST VOICES}
(see @ref{Information retrieval commands}).  See also @ref{Standard
voices} for list of hardwired voice names.

@item SET @{ all | self | @var{id} @} RATE @var{n}
Set the rate of speech.  @var{n} is an integer value within the range
from -100 to 100, with 0 corresponding to the default rate of the
current speech synthesis output module, lower values meaning slower
speech and higher values meaning faster speech.

@item SET @{ all | self | @var{id} @} PITCH @var{n}
Set the pitch of speech.  @var{n} is an integer value within the range
from -100 to 100, with 0 corresponding to the default pitch of the
current speech synthesis output module, lower values meaning lower
pitch and higher values meaning higher pitch.

@item SET @{ all | self | @var{id} @} PUNCTUATION @{ all | some | none @}
Set punctuation mode to the given value.  @code{all} means read all
punctuation characters, @code{none} read no punctuation characters,
@code{some} means read only punctuation characters given in the
server configuration or defined by the client's last @code{SET
IMPORTANT_PUNCTUATION} command.

@item SET @{ all | self | @var{id} @} IMPORTANT_PUNCTUATION @var{chars}
Set punctuation characters read when @code{SET PUNCTUATION some} is
set to those in @var{chars}.  @var{chars} is a sequence of the
required characters, without any spaces.  @var{char} may not contain
control characters and may not begin with double quotes.

@item SET @{ all | self | @var{id} @} CAP_LET_RECOGN @{ none | spell | icon @}
Set capital letters recognition mode. @code{none} switches this
feature off. @code{spell} causes that capital letters are spelled
in the output using the table set as @code{CAP_LET_RECOGN_TABLE}.
With parameter @code{icon}, each capital letter will be preceeded
by a sound icon (either sound or textual) specified by the user
in his configuration.

@item SET @{ all | self | @var{id} @} @var{table}  @var{name}
There are various tables, to allow customization of spelling,
punctuation and other text transformation features.  There are
standard tables defined internally (such as @ref{Standard spelling
tables} or @ref{Standard sound tables}) or you can define your own
tables and add them to Speech Dispatcher through configuration.

@var{table} must be one of the tables listed below.
@itemize @bullet
@item PUNCTUATION_TABLE
@item SPELLING_TABLE
@item TEXT_TABLE
@item SOUND_TABLE
@item CHARACTER_TABLE
@item KEY_TABLE
@item CAP_LET_RECOGN_TABLE
@end itemize

@var{name} must be one of table names returned to the corresponding
@code{LIST} command, see @ref{Information retrieval commands}.

@item SET @{ all | self | @var{id} @} HISTORY @{ on | off @}
Enable (@code{on}) or disable (@code{off}) storing of received
messages into history.

This command is intended for use by message history browsers and
usually should not be used by other kinds of clients.
@end table


@node Information retrieval commands, History handling commands, Parameter setting commands, SSIP commands
@subsection Retrieving information

The @code{LIST} command serves for retrieving information that can be
presented to the user for selection of the values to the @code{SET}
command.  The information listed is selected according to the first
argument of the @code{LIST} command.

@table @code
@item LIST SPELLING_TABLES
List the names of all the text spelling tables available on the
server.  Each table name is listed on a separate line.  Each name may
contain only alphanumeric characters and underscores.

Example Speech Dispatcher response:

@example
200-sptable2
200-sptable1
200-sptable44
200-special_table
200 OK Tables listed.
@end example

The standard sound tables are listed only if they are mapped
to some real table, see @ref{Standard sound tables}.

@item LIST PUNCTUATION_TABLES
Similar to @code{LIST SPELLING_TABLES}, but lists the names of the
available punctuation spelling tables.

@item LIST TEXT_TABLES
Similar to @code{LIST SPELLING_TABLES}, but lists the names of the
available text mapping tables.

@item LIST SOUND_TABLES
Similar to @code{LIST SPELLING_TABLES}, but lists the names of the
available sound mapping tables.

The standard sound tables are listed only if they are mapped
to some real table, see @ref{Standard sound tables}.

@item LIST CHARACTER_TABLES
Similar to @code{LIST SPELLING_TABLES}, but lists the names of the
available character spelling tables.

@item LIST KEY_TABLES
Similar to @code{LIST SPELLING_TABLES}, but lists the names of the
available key description tables.

@item LIST CAP_LET_RECOGN_TABLES
Similar to @code{LIST SPELLING_TABLES}, but lists the names of the
tables available for use with capital recognition.


@item LIST VOICES
Similar to @code{LIST SPELLING_TABLES}, but lists the available voice
names.

The standard voices are always listed, see @ref{Standard voices}.
@end table

@node History handling commands, Other commands, Information retrieval commands, SSIP commands
@subsection History handling

History is handled by the @code{HISTORY} command.  It can take many
forms, described below, that allow browsing, retrieving and repeating
stored messages.  In each invocation of the @code{HISTORY} command
there is no difference between processing spoken or not spoken
messages, all the received messages are processed.

There can be @dfn{history cursor} pointing on some message in the
history.  You can move it across history messages and retrieve the
message the cursor is pointing to, using the @code{HISTORY CURSOR} set
of command arguments described below.

@table @code
@item HISTORY GET CLIENT_LIST
List known client names, their identifiers and status.  Each connection is
listed on a separate line in the following format:

@example
@var{id} @var{name} @var{status}
@end example

where @var{id} is a client id that can be used in other history
handling requests or in the speech output control commands
(@pxref{Speech output control commands}), @var{name} is the client
name as set through the @code{SET CLIENT_NAME} command, and
@var{status} is @code{1} for connected clients and @code{0} for
disconnected clients.  @var{id}s are unique within a single run of
Speech Dispatcher.

Sample reply of Speech Dispatcher:

@example
240-0 joe:speechd_client:main 0
240-1 joe:speechd_client:status 0
240-2 unknown:unknown:unknown 1
240 OK CLIENTS LIST SENT
@end example

@item HISTORY GET CLIENT_ID
Return id of the client itself.

The id is listed on a separate line in the following format:

@example
@var{id}
@end example

Example:

@example
200-123
200 OK CLIENT ID SENT
@end example

@item HISTORY GET CLIENT_MESSAGES @{ @var{id} | all | self @} @var{start} @var{number}
List identifiers of messages sent by the client identified by
@var{id}.  If the special identifier @code{all} is used, identifiers
of messages sent by all clients are listed; if the special identifier
@code{self} is used, identifiers of messages sent by this client are
listed.

@var{number} of messages is listed, starting from the message numbered
@var{start}.  Both @var{number} and @var{start} must be positive
integers.  The first message is numbered 1, the second 2, etc.  If the
given range exceeds the range of available messages, no error is
signaled and the given range is restricted to the available range of
messages.

Messages are sorted by the criterion used in the last client's
invocation of the @code{HISTORY SORT} command.  If no @code{HISTORY
SET} has been invoked yet, the messages are sorted from the oldest to
the newest, according to their time of arrival to Speech Dispatcher.

Each message id is listed, together with other information, on a
separate line, in the following format:

@example
@var{id} @var{client-id} @var{client-name} "@var{time}" @var{priority} "@var{intro}"
@end example

@var{client-id} is a numeric identifier of the client which sent the
message, @var{client-name} is its name as set by the @code{SET
CLIENT_NAME} command, see @ref{Parameter setting commands}.
@var{time} is the time of arrival of the message, in the fixed length
@code{YYYY-MM-DD HH:MM:SS} format.  @var{priority} is the priority of
the message, one of the values accepted by the @code{SET PRIORITY}
command, see @ref{Parameter setting commands}.

@var{intro} is the introductory part of the message of a certain
maximum length, see the @code{HISTORY SET SHORT_MESSAGE_LENGTH}
command.  @var{intro} does not contain any double quotes nor the line
feed character.

All the message identifiers in the history, regardless of clients that
issued them, are unique within a single run of Speech Dispatcher and
remain unchanged.

@item HISTORY GET LAST
List the id of the last message sent by the client.

The id is listed on a separate line of the following format:

@example
@var{id}
@end example

If the client haven't sent any message yet, return an error code.

@item HISTORY GET MESSAGE @var{id}
Return the text of the history message identified by @var{id}.  If
@var{id} doesn't refer any message, return an error code instead.
The text is sent as a multi-line message, with no escaping or special
transformation.

An example SSIP response to the command:

@example
200-Hello, world!
200-How are you?
200 OK MESSAGE SENT
@end example

@item HISTORY CURSOR GET
Get the id of the message the history cursor is pointing on.

The id is listed on a separate line.  Sample Speech Dispatcher reply to
this command:

@example
243-42
243 OK CURSOR POSITION RETURNED
@end example

@item HISTORY CURSOR SET @{ @var{id} | all | self @} @{ first | last | pos @var{n} @}
Set the history cursor to the given position.  The meaning of the
first argument after @code{SET} is the same as in the @code{HISTORY
GET CLIENT_MESSAGES} command.  The argument @code{first} asks to set
the cursor on the first position and the argument @code{last} asks to
set the cursor on the last position of the history of the given
client.  If the argument @code{pos} is used, the position is set to
@var{n}, where @var{n} is a positive integer.  It is an error if
@var{id} doesn't identify any client or if @var{n} doesn't point to
any existing position in the history.

As for the order and numbering of the messages in the history, the
same rules apply as in @code{HISTORY GET CLIENT_MESSAGES}, see above.

@item HISTORY CURSOR @{ forward | backward @}
Move the cursor one position @code{forward}, resp. @code{backward},
within the messages of the client specified in the last @code{HISTORY
CURSOR SET} command.  If there is no next, resp. previous, message,
don't move the cursor and return an error code.

@item HISTORY SAY @var{id}
Speak the message from history identified by @var{id}.  If @var{id}
doesn't refer any message, return an error code instead.

The message is spoken as it would be sent by its originating command
(@code{SPEAK} or @code{SOUND_ICON}), but the @emph{current} settings
(priority, etc.) apply.

@item HISTORY SORT @{ asc | desc @} @{ time | user | client_name | priority | message_type @}
Sort the messages in history according to the given criteria.  If the
second command argument is @code{asc}, sort in the ascending order, if
it is @code{desc}, sort in the descending order.  The third command
argument specifies the message property to order by:

@table @code
@item time
Time of arrival of the message.

@item user
User name.

@item client_name
Client name, excluding user name.

@item priority
Priority.

@item message_type
Type of the message (text, sound icon, character, key), in the order
specified in the Speech Dispatcher configuration or by the @code{HISTORY
SET MESSAGE_TYPE_ORDERING} command.
@end table

The sorting is stable --- order of all the messages that are equal in
the given ordering remains the same.

The sorting is specific to the given client connection, other
connections are unaffected by invocation of this command.

@item HISTORY SET SHORT_MESSAGE_LENGTH @var{length}
Set the maximum length of short versions of history messages to
@var{length} characters.  @var{length} must be a non-negative integer.

Short (truncated) versions of history messages are used e.g. in the
answer to the @code{HISTORY GET CLIENT_MESSAGES} format.

@item HISTORY SET MESSAGE_TYPE_ORDERING "@var{ordering}"
Set the ordering of the message types, from the minimum to the
maximum.  @var{ordering} is a sequence of the following symbols,
separated by spaces: @code{text}, @code{sound_icon}, @code{char},
@code{key}.  The symbols are case insensitive and each of them must be
present in @var{ordering} exactly once.

The specified ordering can be used by the @code{HISTORY SORT} command.

@item HISTORY SEARCH @{ @var{id} | all | self @} "@var{condition}"
Return the list of history messages satisfying @var{condition}.  The
command allows searching messages by given words.  The output format
is the same as of the @code{HISTORY GET CLIENT_MESSAGES} command.

The meaning of the first argument after @code{SEARCH} is the same as
in the @code{HISTORY GET CLIENT_MESSAGES} command.

@var{condition} is constructed according to the following grammar
rules:

@table @code
@item @var{condition} :: @var{word}
Matches messages containing @var{word}.

@item @var{condition} :: ( ! @var{condition} )
Negation of the given condition.

@item @var{condition} :: ( @var{condition} [ & @var{condition} ... ] )
Logical AND --- all the conditions must be satisfied.

@item @var{condition} :: ( @var{condition} [ | @var{condition} ... ] )
Logical OR --- at least one of the conditions must be satisfied.
@end table

Spaces within the condition are insignificant and ignored.

The following rules apply to @var{word}s:

@itemize @minus
@item
@var{word} is a sequence of adjacent alphanumeric characters.

@item
If @var{word} contains any upper-case letter, the search for the word
is case sensitive, otherwise it's case insensitive.

@item
@var{word} must match whole word, not only its substring.

@item
@var{word} can contain the wild card characters @code{?}, substituting
any single alphanumeric character, and @code{*}, substituting any
number (incl. zero) of alphanumeric characters.
@end itemize

Returned messages are sorted by the following rules:

@enumerate
@item
The primary sorting is defined by the number of the satisfied
subconditions on the top level of the given condition, from the
highest (best matching messages first) to the lowest.  This takes any
effect only if the given condition is the OR rule.

@item
The criterion used in the last client's invocation of the
@code{HISTORY SORT} command.  If no @code{HISTORY SORT} has been
invoked yet, the messages are sorted from the oldest to the newest,
according to their time of arrival to Speech Dispatcher.
@end enumerate
@end table

@node Other commands,  , History handling commands, SSIP commands
@subsection Other commands

@table @code
@item QUIT
Close the connection.

@item HELP
Print a short list of all SSIP commands, as a multi-line message.
@end table

@node Return codes, Sample SSIP relation, SSIP commands, SSIP
@section Return codes

Each line of the SSIP output starts with a three-digit numeric code of
the form @var{NXX} where @var{N} determines the result group and
@var{xx} denotes the finer classification of the result.

SSIP defines the following result groups:

@table @var
@item 1xx
Informative response --- general information about the protocol, help
messages.

@item 2xx
Operation was completely successful.

@item 3xx
Server error, problem on the server side.

@item 4xx
Client error, invalid arguments or parameters received.

@item 5xx
Client error, invalid command syntax, unparseable input.
@end table

Result groups @var{1xx} and @var{2xx} correspond to successful
actions, other groups to unsuccessful actions.  Only the groups
defined here may be returned from the Speech Dispatcher.

Currently, only the meaning of the first digit of the result code is
defined, the last two digits are insignificant and can be of any
value.  Clients shouldn't rely on the unspecified digits in any way.
If you are going to write your own SSIP implementation, please consult
the authors of Speech Dispatcher to define more precise set of return
codes.

@node Sample SSIP relation,  , Return codes, SSIP
@section Example of an SSIP relation

The following example illustrates a sample relation with SSIP.  The
client connects to the Speech Dispatcher, sets all the common parameters,
sends two text messages, displays the list of clients, instructs
Speech Dispatcher to repeat the second message, and closes the connection.
Lines starting with a numeric code are response lines of the server,
other lines are the lines sent by the client.

@example
SET SELF CLIENT_NAME joe:vi:default
208 OK CLIENT NAME SET
SET SELF PRIORITY MESSAGE
202 OK PRIORITY SET
SPEAK
230 OK RECEIVING DATA
Hello, I'm a Speech Dispatcher communication example!
How are you?
.
225 OK MESSAGE QUEUED
SPEAK
230 OK RECEIVING DATA
Still there?
.
225 OK MESSAGE QUEUED
HISTORY GET CLIENT_LIST
240-1 jim:Emacs:default 0
240-2 jim:Emacs:default 0
240-3 unknown:unknown:unknown 0
240-4 jim:Emacs:default 1
240-5 joe:vi:default 1
240 OK CLIENTS LIST SENT
HISTORY GET LAST
242-39 joe:vi:default
242 OK LAST MSG SAID
QUIT
231 HAPPY HACKING
@end example


@node Priorities, Multiple output modules, SSIP, Top
@chapter Priorities
@cindex priorities

Speech Dispatcher can't synthesize everything that comes to it,
for the simple reason, that messages are often coming faster
than a synthetic voice can say them. On the screen of a
monitor, there is relatively a lot of space compared to
one-channel speech synthesis output. For this reason, we
use a system of several priorities targeted at different
types of messages.

@menu
* Description of priorities::   What are the available priorities 
* Examples of using priorities::  When to use which priority
@end menu

@node Description of priorities, Examples of using priorities, Priorities, Priorities
@section Description of priorities
                     
Speech Dispatcher provides the system of five priorities.
Every message will either contain explicit priority information,
or the default value will be considered.

Please see also the diagram bellow.

@subsection Priority @code{important}
@cindex Priority text

This message will be said immediately as it comes to server.
It is never interrupted. When several concurrent messages are received by
server, they are queued and said in the order, they came.

When a new message of level @code{important} comes during lower level
message is spoken, lower level message is canceled and removed
from the queue.

These messages should be as short as possible and should
be rarely used, because they block the output of all other
messages.

@subsection Priority @code{message}
@cindex Priority text

This message will be said when there is no message of priority
@code{important}, @code{text} or @code{message} waiting in the queue.
If more messages with priority @code{message} come, they are queued
and said in the order they were received, so to say, priority
text doesn't interrupt itself. Messages of priorities @code{notification}
and @code{progress} are thrown away.

@subsection Priority @code{text}

This message will be said when there is no message of priority
@code{important}, @code{message} or @code{text} waiting in the queue.
Each time a priority @code{text} message arrives and there is
some other message of this priority queued or being spoken,
this message is canceled and the new one is spoken. Priority
text interrupts itself. Messages of priorities @code{notification}
and @code{progress} are thrown away.

@subsection Priority @code{notification}

This is a low priority message. If there are messages with priorities
@code{important}, @code{messages}, @code{text} or @code{progress} waiting in the
queues or being spoken, this @code{notification} message is canceled.

This priority interrupts itself, so if more messages with priority
@code{notification} come at the same time, only the last of them is
spoken.

@subsection Priority @code{progress}

This is a special priority for messages that are coming
shortly one after each other and they carry the information
about some work in progress (e.g. @code{Completed 45%}).
If each new message would interrupt each other (see priority
Notification), the user would hear none of the message
till the end.

This priority behaves the same as ``notification'' except
for 2 things:

@itemize
   @item The messages of this priority don't interrupt
each other, instead, they are canceled if some other message
is being spoken.
   @item It tries to detect the last message of a series
of messages (It's important for the user to hear the
@code{Completed 100%} message to know that the process has
terminated). It does so by setting an alarm for a specified
amount of time when receiving each message. If no other message
comes in this time, the message is considered the last and
it's sent to synthesis.
@end itemize

@subsection Diagram of the priorities

@image{figures/priorities,,,Speech Dispatcher architecture (you can see a text
version of this figure in the Info manual)}

@node Examples of using priorities,  , Description of priorities, Priorities
@section Examples of using priorities

Example uses for priority @code{important} are:
 
@itemize
@item error messages
@item very important messages
@item ...
@end itemize

Example uses for level @code{message} are:

@itemize
@item regular program messages
@item warnings
@item ...
@end itemize

Example uses for level @code{text} are:

@itemize
@item text the user is working on
@item menu items
@item ...
@end itemize

Example uses for level @code{notification} are:

@itemize
@item less important status information
@item letters when typing input
@item ...
@end itemize

Example uses for level @code{progress} are:

@itemize
@item ``completed 15%''
@item ...
@end itemize


@node Multiple output modules, Message history, Priorities, Top
@chapter Multiple output modules
@cindex output module
@cindex different synthesizers

Speech Dispatcher supports concurrent use of multiple output modules.
In the case these output modules provide good synchronization,
you can combine them in reading messages. For example if module1 can
speak English and Czech while module2 speaks only German, the idea
is that if there is something message in German, module2 is used,
while module1 is used for the other languages. These rules for
selection of output modules can be influenced through the configuration
file @file{speechd.conf}.

If you want to compile and use a new output module, you should place
it in @file{src/modules} in your source directory of Speech Dispatcher and
add it to @file{src/modules/Makefile.am}. You can compile and install
it by typing: @code{make; su root; make install}. The last step you
have to do is to let Speech Dispatcher know you want to use this new
module by adding a line to @file{speechd.conf} in your configuration directory
@example
AddModule module_name
@end example
and possibly also changing the line
@example
DefaultModule new_module
@end example
to make it default.

@xref{Output modules}.

@node Message history, Speech parameters, Multiple output modules, Top
@chapter Message history
@cindex history

@menu
* Access rights::               Access rights to the history messages.
@end menu

@node Access rights,  , Message history, Message history
@section Access rights
@cindex access rights

To protect privacy of users, Speech Dispatcher restricts history access to
a certain subset of all the received messages.  The following rules
apply:

@itemize @bullet
@item
All the messages issued by a client connection are accessible to that
client connection.

@item
All the messages sent by a given user are accessible to that user.

@item
@cindex @code{speechd} user
@cindex @code{speechd} group
All the messages sent by the user @code{speechd} are accessible to all
users on the system running the Speech Dispatcher instance present in the
group @code{speechd}.

@item
No other messages are accessible.
@end itemize

@cindex Identification Protocol
@cindex identd
@cindex RFC 1413
Two users are considered the same, if and only if their connections
originate on the same host, their user names are the same, and their
identity can be checked, as described bellow.  Speech Dispatcher does not
provide any explicit authentication mechanism.  To check the identity
of users, Speech Dispatcher uses the Identification Protocol mechanism
defined by RFC 1413 to get the user's identity.  If user's identity
cannot be checked, the user is considered different of all other
connected or previously connected users.

@cindex user mapping
Speech Dispatcher allows to specify user mapping in its configuration,
allowing to change certain users to different users, see
@ref{Configuration}.


@node Speech parameters, Configuration, Message history, Top
@chapter Speech parameters
@cindex Speech parameters
@cindex Settings

@section Language selection

Various synthesizers provide different sets of possible
languages, they are allowed to speak. We must be able to
receive a request for setting particular language (using
ISO language code) and reply, if the language is supported.

@section Speed

Sped of the speech is supported by all synthesizers, but the
values and their ranges differ. Each output module is
responsible to set the speed to the value, best responding to
current setting. This may be a little bit difficult, because
there is no exact scale. We could take some longer English
paragraph and take it as a base for our new scale. If this
paragraph is said in e.g. ten seconds, this means speed = 100,
if it is said in twenty seconds, speed = 200. This way, we
can coordinate  different scales quite precisely (the paragraph
should be long enough).

@section Punctuation mode

Punctuation mode describes the way, in which the synthesizer
works with non-alphanumeric characters. Most synthesizers
support several punctuation modes. We will support a reasonable
superset of those modes, which may be implemented in device
driver, when not supported by hardware.

@section Pitch
Pitch is the voice frequency. We face the similar problems
here, as with Speed setting.

@section Voice type
Most synthesizers provide several voice types, such as male,
female, child etc. The set is again different for each
of the devices. Speech Dispatcher should try to find the nearest
possible (if the request is child female and it's not available,
we will try to use adult female rather then adult male).

@section Spelling
Spelling mode is provided by nearly all devices and is also
easy to emulate in output module.

@section Capital letters recognition
That is again a widely supported feature. However it is
desirable to support this internally, using the
sound icons feature, but this  requires a good possibility of
synchronization, which is not  possible with all devices.


@node Configuration, Key names, Speech parameters, Top
@chapter Configuration
@cindex configuration
@cindex default values

Speech Dispatcher can be configured on several levels.
There is a configuration file where permanent settings
are stored, but user and applications can also change
the majority of parameters on-fly by calling Speech Dispatcher
functions. The third level of configuration can't be
changed and it's given by the capabilities of each output
device (each output module for each output device reports
it's capabilities when it's loaded into Speech Dispatcher).

We use DotConf for the permanent text file based configuration.
See @file{speechd.conf}.

Other parts of this manual deal with the runtime configuration.

@section Verbosity

There are 6 different verbosity levels of Speech Dispatcher logging.
0 means there is no output, while 5 means that nearly all the information
about Speech Dispatcher working is written to standard output.

@itemize @bullet

@item Level 0
@itemize @bullet
@item No information.
@end itemize

@item Level 1
@itemize @bullet
@item Information about loading and exiting.
@end itemize

@item Level 2
@itemize @bullet
@item Information about errors that occurred.
@item Allocating and freeing resources on start and exit.
@end itemize

@item Level 3
@itemize @bullet
@item Information about accepting/rejecting/closing clients' connections.
@item Information about invalid client commands.
@end itemize

@item Level 4
@itemize @bullet
@item Every received command is output.
@item Information about proceeding the command output
@item Information about queueing/allocating messages.
@item Information about the function of history, sound icons and other
facilities.
@item Information about the work of the speak() thread.
@end itemize

@item Level 5

(This is only for debugging purposes and can output really *much*
data. Use with caution.)
@itemize @bullet
@item Also received data (messages etc.) is output.
@end itemize

@end itemize


@node Key names, Standard sound icons, Configuration, Top
@appendix Key names

This appendix defines all the recognized symbolic key names.  The
names are case sensitive.

@menu
* Auxiliary key names::         Shift, Meta, etc.
* Control character key names::  Return, LineFeed, etc.
* Special key names::           Space, underscore, etc.
@end menu

@node Auxiliary key names, Control character key names, Key names, Key names
@section Auxiliary keys

@table @code
@item control
@item hyper
@item meta
@item shift
@item super
@end table

@node Control character key names, Special key names, Auxiliary key names, Key names
@section Control character keys

@table @code
@item backspace
@item break
@item delete
@item down
@item end
@item enter
@item escape
@item f1
@item f2
@item f3
@item f4
@item f5
@item f6
@item f7
@item f8
@item f9
@item f10
@item f11
@item f12
@item f13
@item f14
@item f15
@item f16
@item f17
@item f18
@item f19
@item f20
@item f21
@item f22
@item f23
@item f24
@item home
@item insert
@item kp-*
@item kp-+
@item kp--
@item kp-.
@item kp-/
@item kp-0
@item kp-1
@item kp-2
@item kp-3
@item kp-4
@item kp-5
@item kp-6
@item kp-7
@item kp-8
@item kp-9
@item kp-enter
@item left
@item menu
@item next
@item num-lock
@item pause
@item print
@item prior
@item return
@item right
@item scroll-lock
@item space
@item tab
@item up
@item window
@end table

@node Special key names,  , Control character key names, Key names
@section Special key names

@table @code
@item space
@item underscore
@item double-quote
@end table


@node Standard sound icons, Standard spelling tables, Key names, Top
@appendix Standard sound icons

There are none currently.


@node Standard spelling tables, Standard sound tables, Standard sound icons, Top
@appendix Standard spelling tables

The following spelling table names are always present in the output of
the @code{LIST SPELLING tables} command (@pxref{Information retrieval
commands}):

@table @code
@item spelling-short
@item spelling-long
@end table


@node Standard sound tables, Standard voices, Standard spelling tables, Top
@appendix Standard sound tables

There are none currently.


@node Standard voices, GNU General Public License, Standard sound tables, Top
@appendix Standard voices

The following voice names are always present in the output of the
@code{LIST VOICES} command (@pxref{Information retrieval commands}):

@table @code
@item MALE1
@item MALE2
@item MALE3
@item FEMALE1
@item FEMALE2
@item FEMALE3
@item CHILD_MALE
@item CHILD_FEMALE
@end table

The actual presence of any of these voices is not guaranteed.  But the
command @code{SET VOICE} (@pxref{Parameter setting commands}) must
accept any of them.  If the given voice is not available, it is mapped
to another voice by the output module.


@node GNU General Public License, GNU Free Documentation License, Standard voices, Top
@appendix GNU General Public License
@center Version 2, June 1991
@cindex GPL, GNU General Public License

@include gpl.texi


@node GNU Free Documentation License, Concept index, GNU General Public License, Top
@appendix GNU Free Documentation License
@center Version 1.2, November 2002
@cindex FDL, GNU Free Documentation License

@include fdl.texi


@node Concept index,  , GNU Free Documentation License, Top
@unnumbered Concept index

@cindex tail recursion
@printindex cp

@contents
@bye

@c  LocalWords:  texinfo setfilename speechd settitle finalout syncodeindex pg
@c  LocalWords:  setchapternewpage cp fn vr texi dircategory direntry titlepage
@c  LocalWords:  Cerha Hynek Hanke vskip pt filll insertcopying ifnottex dir fd
@c  LocalWords:  API SSIP cindex printf ISA pindex Flite Odmluva FreeTTS TTS CR
@c  LocalWords:  ViaVoice Lite Tcl Zandt wxWindows AWT spd dfn backend findex
@c  LocalWords:  src struct gchar gint const OutputModule intl FDSetElement len
@c  LocalWords:  fdset init flite deffn TFDSetElement var int enum EVoiceType
@c  LocalWords:  sayf ifinfo verbatiminclude ref UTF ccc ddd pxref LF cs conf
@c  LocalWords:  su AddModule DefaultModule xref identd printindex Dectalk GTK

@c speechd.texi ends here
@c  LocalWords:  emph soundcard precission archieved succes
